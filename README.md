# The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist

Scientific innovation is undergoing a paradigm shift driven by the rapid advancement of Large Language Models (LLMs). As science faces mounting challenges including information overload, disciplinary silos, and diminishing returns on conventional research methods, LLMs are emerging as powerful agents capable not only of enhancing scientific workflows but also of participating in and potentially leading the innovation process. Existing surveys mainly focus on different perspectives, phrases, and tasks in scientific research and discovery, while they have limitations in understanding the transformative potential and role differentiation of LLM. This survey proposes a comprehensive framework to categorize the evolving roles of LLMs in scientific innovation across three hierarchical levels: Evaluator, Collaborator, and Scientist. We distinguish between LLMs’ contributions to structured scientific research processes and open-ended scientific discovery, thereby offering a unified taxonomy that clarifies capability boundaries, evaluation criteria, and human-AI interaction patterns at each level. Through an extensive analysis of current methodologies, benchmarks, systems, and evaluation metrics, this survey delivers an in-depth and systematic synthesis on LLM-driven scientific innovation. We present LLMs not only as tools for automating existing processes, but also as catalysts capable of reshaping the epistemological foundations of science itself. This survey offers conceptual clarity, practical guidance, and theoretical foundations for future research, while also highlighting open challenges and ethical considerations in the pursuit of increasingly autonomous AI-driven science.

<img width="823" height="362" alt="Screenshot 2025-07-13 at 2 41 32 PM" src="https://github.com/user-attachments/assets/9f889e1f-b076-47be-8c9a-e814a5177249" />

Figure 3. The pyramidal framework of large language models' roles in scientific innovation: evaluators, collaborators, scientists.

## Contents

*   [Level 1: LLMs as Evaluators](#level-1-LLMs-as-Evaluators)
    *   [Scientific Knowledge Synthesis](#Scientific-Knowledge-Synthesis)
    *   [Scientific Literature Quality Assessment](#Scientific-Literature-Quality-Assessment)
*   [Level 2: LLMs as Collaborators](#level-2-LLMs-as-Collaborators)
    *   [Hypothesis Generation](#Hypothesis-Generation)
    *   [Experimental Assistant](#Experimental-Assistant)
*   [Level 3: LLMs as Scientists](#level-3-LLMs-as-Scientists)
    *   [Autonomous Scientific Research](#Autonomous-Scientific-Research)
    *   [Autonomous Scientific Discovery](#Autonomous-Scientific-Discovery)
 
## Level 1: LLMs as Evaluators

<img width="768" height="343" alt="Screenshot 2025-07-13 at 2 49 29 PM" src="https://github.com/user-attachments/assets/75a7a0b0-fc18-4be0-8e0c-47280c5180f4" />

Figure 5: Closed-loop workflow of LLMs as Evaluators. Multimodal embeddings underpin SKS (blue) and SLQA (green), whose outputs mutually reinforce each other and update the shared database.

### Scientific Knowledge Synthesis

#### Benchmarks

#### Algorithms

### Scientific Literature Quality Assessment

#### Benchmarks

#### Algorithms

## Level 2: LLMs as Collaborators

### Hypothesis Generation

#### Benchmarks

#### Algorithms

### Experimental Assistant

#### Benchmarks

#### Algorithms

## Level 3: LLMs as Scientists

### Autonomous Scientific Research

#### Benchmarks

#### Algorithms

### Autonomous Scientific Discovery

#### Benchmarks

#### Algorithms


